# NeuralScribe

**Real-time handwritten character recognition with live neural network visualization.**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-3776ab?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-22d3ee?style=flat-square)](LICENSE)

<!-- 
ğŸ“¸ HERO IMAGE â€” Full dashboard, dark theme, character drawn, prediction active.
   Draw "R" or "7" so network viz + confidence bars light up.
   Save as: assets/hero.png (1920Ã—1080)
-->
![NeuralScribe Dashboard](assets/hero.png)

---

## What Is This?

NeuralScribe trains a CNN on the **EMNIST ByMerge** dataset â€” 47 classes covering digits `0â€“9`, uppercase `Aâ€“Z`, and 11 lowercase letters `a b d e f g h n q r t`. You draw a character on a canvas, and the neural network predicts what you wrote in real time, updating mid-stroke with no button clicks. A live visualization shows the 6-layer CNN activating as predictions form.

<!-- 
ğŸ¬ DEMO GIF â€” 8â€“12 seconds: draw â†’ prediction live â†’ clear â†’ draw another.
   720p, 15fps. Tools: ScreenToGif / Kap / Peek
   Save as: assets/demo.gif
-->
![Live Demo](assets/demo.gif)

---

## Features

| Feature | Detail |
|---------|--------|
| **Live inference** | Predictions update mid-stroke via WebSocket. A pixel queue ensures continuous predictions â€” drawing is never blocked by pending responses |
| **6-layer network visualization** | Watch Input â†’ Conv1 â†’ Conv2 â†’ Dense1 â†’ Dense2 â†’ Output(47) activate. Green = predicted path, red = competing activations |
| **Train from the UI** | Click Train, set 5â€“100 epochs with a slider, watch accuracy and loss charts update each epoch with ETA countdown |
| **Dark & light themes** | Toggle with one click, preference saved to localStorage. Smooth 350ms transitions between themes |
| **47-class confidence bars** | Full probability distribution â€” digits in cyan, letters in violet. Winning class highlighted with glow effect |
| **Stroke undo** | Undo button pops the last stroke. Full stroke history preserved |
| **Test-time augmentation** | 5 inference variants (original + Â±4Â° rotation + 1px shifts) averaged for more robust predictions |
| **GPU acceleration** | Auto-detects CUDA. Mixed precision training (fp16) for ~1.5â€“2x speedup on RTX GPUs |

---

## Quick Start

```bash
# Clone
git clone https://github.com/yourusername/neuralscribe.git
cd neuralscribe

# Environment
python -m venv .venv
.venv\Scripts\activate
python.exe -m pip install --upgrade pip

# Dependencies
pip install -r requirements.txt

# GPU support (CUDA 12.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128

# Launch
python run_backend.py
```

Open **http://localhost:8000** â†’ Click **Train** â†’ Set 35 epochs â†’ Start â†’ Draw.

> First run downloads EMNIST (~500 MB). Training takes ~10â€“20 min on GPU, longer on CPU.

<!-- 
ğŸ“¸ TRAINING â€” Capture mid-training: progress bar with ETA, accuracy chart building,
   loss declining, "TRAINING" status pill in amber.
   Save as: assets/training.png
-->
![Training In Progress](assets/training.png)

---

## Project Structure

```
neuralscribe/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Dashboard page
â”‚   â”œâ”€â”€ style.css               # Design system (dark + light themes)
â”‚   â”œâ”€â”€ main.js                 # Entry point â€” WebSocket, DOM bindings
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â””â”€â”€ appState.js         # Reactive state store (pub/sub)
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ theme.js            # Dark/light toggle + localStorage
â”‚       â”œâ”€â”€ canvas.js           # Drawing, undo stack, pixel extraction
â”‚       â”œâ”€â”€ networkViz.js       # 6-layer CNN visualization (cached wireframe)
â”‚       â””â”€â”€ charts.js           # Accuracy + Loss chart rendering
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py              # FastAPI server + WebSocket
â”‚   â”œâ”€â”€ interface/
â”‚   â”‚   â”œâ”€â”€ preprocess.py       # Canvas â†’ EMNIST tensor pipeline
â”‚   â”‚   â””â”€â”€ predictor.py        # Inference engine with TTA
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ model.py            # CNN architecture (ResBlocks + SE attention)
â”‚   â”‚   â”œâ”€â”€ dataset.py          # EMNIST data loader + augmentation
â”‚   â”‚   â””â”€â”€ train.py            # Training loop (mixed precision, warm restarts)
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ digit_model.pt      # Trained weights (generated after training)
â”œâ”€â”€ data/raw/emnist/            # EMNIST data (auto-downloaded)
â”œâ”€â”€ run_backend.py              # Server launcher
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## How It Works

### Data Flow

```
Draw on canvas
     â”‚
     â–¼
canvas.js extracts 28Ã—28 pixels (bounding box crop + scale + 90Â° rotation)
     â”‚
     â–¼
main.js sends pixels via WebSocket â”€â”€â”€â”€â”€â”€â–º backend app.py receives
                                                    â”‚
                                                    â–¼
                                           preprocess.py:
                                             1. Normalize to [0,1]
                                             2. Gaussian smoothing
                                             3. Center-of-mass alignment
                                             4. EMNIST transpose
                                             5. EMNIST normalization (Î¼=0.175, Ïƒ=0.333)
                                                    â”‚
                                                    â–¼
                                           predictor.py:
                                             1. Create 5 TTA variants
                                                (original, Â±4Â° rotation, 1px shifts)
                                             2. Forward pass through CNN
                                             3. Average probabilities
                                             4. Extract activations for viz
                                                    â”‚
                                                    â–¼
WebSocket returns prediction â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ JSON response:
     â”‚                                      label, confidence, probabilities[47],
     â–¼                                      activations{conv1,conv2,fc1}, inference_ms
appState.update({ prediction })
     â”‚
     â”œâ”€â”€â–º networkViz.js redraws (from wireframe cache)
     â”œâ”€â”€â–º confidence bars update (47 bars)
     â””â”€â”€â–º prediction display updates (character, confidence, ms)
```

<!-- 
ğŸ¬ NETWORK VIZ GIF â€” 5 seconds, cropped to just the network panel.
   Draw one character â†’ clear â†’ draw another. Connections shift between predictions.
   Save as: assets/network-viz.gif
-->
![Network Visualization](assets/network-viz.gif)

### Frontend Architecture

The frontend uses **vanilla ES modules** â€” zero dependencies, no build step.

All communication between modules goes through a reactive state store:

```
User action â†’ module â†’ appState.update() â†’ subscribers re-render
```

No module touches another module's DOM. This prevents race conditions when training updates and predictions arrive simultaneously.

**appState.js** is a tiny pub/sub system:

```javascript
subscribe('prediction', (pred) => { /* update DOM */ });
update({ prediction: { label: '7', confidence: 94.2 } });
```

**Performance optimizations:**
- **Wireframe caching** â€” the static network grid is drawn once and stored as `ImageData`. Each frame blits the cache with `putImageData()` then draws only active connections on top.
- **requestAnimationFrame gating** â€” duplicate draw calls are collapsed into one frame.
- **Pixel queue** â€” while waiting for a prediction response, new draw events store the latest pixels. When the response arrives, queued pixels fire immediately. Drawing is never blocked by slow inference.

### WebSocket Protocol

The frontend and backend communicate via JSON messages over a single WebSocket:

| Direction | Type | Purpose |
|-----------|------|---------|
| Client â†’ Server | `predict` | Send 784 pixel values for inference |
| Client â†’ Server | `train` | Start training with `{ epochs: N }` |
| Client â†’ Server | `reset_model` | Delete weights and reset predictor |
| Client â†’ Server | `shutdown` | Kill the backend server |
| Client â†’ Server | `ping` | Keepalive (every 15 seconds) |
| Server â†’ Client | `init` | Model status, device, training history on connect |
| Server â†’ Client | `prediction` | Label, confidence, probabilities, activations, inference_ms |
| Server â†’ Client | `training_started` | Training begun, total epochs |
| Server â†’ Client | `training_update` | Per-epoch: accuracy, train/test loss, epoch time, LR |
| Server â†’ Client | `training_complete` | Final accuracy, full history |
| Server â†’ Client | `training_error` | Error message if training fails |
| Server â†’ Client | `model_reset` | Confirms model deleted |
| Server â†’ Client | `shutdown_ack` | Server shutting down |

REST fallback endpoints: `POST /api/predict`, `GET /api/status`, `POST /api/shutdown`.

---

## Model Architecture

```
Input (1Ã—28Ã—28)
     â”‚
     â–¼
Stem: Conv2d(1â†’32, 3Ã—3) â†’ BatchNorm â†’ ReLU
     â”‚
     â–¼
ResBlock1: 32â†’64, stride 2 â†’ 14Ã—14     (SE attention, drop_path=0.05)
     â”‚
     â–¼
ResBlock2: 64â†’160, stride 2 â†’ 7Ã—7      (SE attention, drop_path=0.10)
     â”‚
     â–¼
ResBlock3: 160â†’320, stride 1 â†’ 7Ã—7     (SE attention, drop_path=0.15)
     â”‚
     â–¼
ResBlock4: 320â†’256, stride 1 â†’ 7Ã—7     (SE attention, drop_path=0.20)
     â”‚
     â–¼
Global Average Pooling â†’ Dropout(0.4) â†’ Linear(256â†’47)
```

**3,348,815 parameters** Â· Mixed precision (fp16 on CUDA)

### Dataset

| | Samples | Batches (batch_size=128) |
|---|---------|---------|
| **Train** | 697,932 | 5,452 |
| **Test** | 116,323 | 455 |

8 DataLoader workers Â· pin_memory Â· persistent_workers

Each **ResBlock** contains:
- Conv 3Ã—3 â†’ BatchNorm â†’ ReLU â†’ Conv 3Ã—3 â†’ BatchNorm â†’ SE Block â†’ residual add â†’ ReLU
- **Skip connection**: 1Ã—1 conv when channels or spatial size change
- **Stochastic depth** (DropPath): randomly skips blocks during training, increasing from 5% to 20% deeper in the network

**SE Block** (Squeeze-and-Excitation): Global average pool â†’ FC down â†’ ReLU â†’ FC up â†’ Sigmoid â†’ channel-wise multiply. Learns which feature channels matter most. Helps distinguish confusable pairs like O/0, l/1/I, S/5.

**Why these design choices:**
- **BatchNorm** after every conv stabilizes training and allows a higher learning rate (0.003)
- **Residual connections** prevent vanishing gradients â€” deeper blocks train as easily as shallow ones
- **Global average pooling** instead of flatten reduces parameters from 12,544 â†’ 256 going into the final FC
- **Stochastic depth** is a regularizer that prevents deeper layers from overfitting
- **4 ResBlocks** with channels 64â†’160â†’320â†’256 gives enough capacity for 47 confusable character classes

---

## Training Pipeline

### Optimizer & Scheduler

- **AdamW** with decoupled weight decay (1e-4) and learning rate 0.003
- **CosineAnnealingWarmRestarts** (T_0=10, T_mult=2): the learning rate follows a cosine curve that resets every 10 epochs, then every 20, then 40. Each restart lets the model escape local minima and explore new solutions. This is why 30â€“50 epochs works better than 15 â€” more restart cycles.

### Loss Function

**Focal Loss** with label smoothing:
- **Focal** (Î³=2.0): down-weights easy examples, focuses training on hard/confusable characters. A confidently correct prediction gets near-zero loss. A confused O/0 prediction gets amplified loss.
- **Label smoothing** (0.1): target for correct class is 0.9 instead of 1.0, remainder spread across other classes. Prevents overconfident outputs that hurt generalization.

<!-- 
ğŸ“¸ ACCURACY â€” After training completes, capture the accuracy chart showing the full curve.
   Save as: assets/accuracy.png
-->
![Accuracy Chart](assets/accuracy.png)

### Data Augmentation

Training images pass through this pipeline (test images do not):

| Transform | Parameters | Purpose |
|-----------|------------|---------|
| RandomAffine | Â±10Â° rotation, Â±10% translate, 90â€“110% scale | Simulates natural drawing variation |
| RandomPerspective | 15% distortion, 30% probability | Simulates viewing angle (phone tilt) |
| GaussianBlur | kernel 3, Ïƒ 0.1â€“0.7 | Simulates different stroke widths |
| RandomErasing | 2â€“25% area, 50% probability | Forces recognition with missing parts |

### Mixup Regularization

During training, random pairs of images are blended: `x = Î»Â·xâ‚ + (1-Î»)Â·xâ‚‚` with labels mixed the same way (Î±=0.2). This smooths decision boundaries between classes and reduces overfitting.

### Mixed Precision

On CUDA GPUs, forward passes run in fp16 (`torch.amp.autocast`) while gradient accumulation stays in fp32. The `GradScaler` prevents fp16 underflow. This gives ~1.5â€“2x speedup on RTX GPUs with tensor cores at no accuracy cost. CPU training runs in fp32 only.

### Gradient Clipping

`clip_grad_norm_(max_norm=1.0)` prevents rare large gradients from destabilizing training.

---

## Preprocessing Pipeline

The canvas-to-model pipeline must exactly match how EMNIST training data is formatted. Any mismatch is a systematic error that no model improvement can fix.

| Step | What | Why |
|------|------|-----|
| 1. Bounding box crop | Crop to drawn content + padding | Remove empty space |
| 2. Scale to 28Ã—28 | Fit into model input size, maintain aspect ratio | Match EMNIST dimensions |
| 3. 90Â° CCW rotation + vertical flip | Correct canvas orientation | Canvas captures in screen orientation; EMNIST has its own |
| 4. Normalize to [0,1] | Divide by 255 | Standard pixel normalization |
| 5. Gaussian smoothing | 3Ã—3 kernel, 70/30 blend with original | Canvas strokes are sharper than EMNIST â€” softening matches the training distribution |
| 6. Center-of-mass alignment | Shift so brightness centroid is at (13.5, 13.5) | EMNIST uses center-of-mass positioning, not bounding-box centering |
| 7. EMNIST transpose | `.T` on the 2D array | Matches `TransposeImage()` applied during training |
| 8. EMNIST normalization | `(pixels - 0.1751) / 0.3332` | Mean/std computed from the EMNIST training set |

### Test-Time Augmentation (TTA)

At inference, 5 variants of the input are created and predictions averaged:

| Variant | Transform |
|---------|-----------|
| 1 | Original (unchanged) |
| 2 | Rotated +4Â° clockwise |
| 3 | Rotated -4Â° counter-clockwise |
| 4 | Shifted 1 pixel right |
| 5 | Shifted 1 pixel down |

These correct for natural drawing variation â€” slightly tilted characters, off-center strokes. Averaging 5 predictions is more robust than a single prediction. Cost is ~5x inference time, but single inference is 1â€“3ms on GPU so TTA still runs in ~5â€“15ms.

TTA can be disabled in `predictor.py` with `use_tta=False` for faster inference.

---

## Design System

<!-- 
ğŸ“¸ DARK + LIGHT SIDE BY SIDE â€” Same character drawn in both themes. Crop identically.
   Save as: assets/dark-theme.png and assets/light-theme.png
-->
| Dark Theme | Light Theme |
|:---:|:---:|
| ![Dark](assets/dark-theme.png) | ![Light](assets/light-theme.png) |

### Typography

| Font | Weight | Used For |
|------|--------|----------|
| **Syne** | 800 | Brand name "Neural" â€” geometric, technical |
| **Caveat** | 700 | Brand name "Scribe" â€” handwriting accent (the theme) |
| **Outfit** | 300â€“700 | Body text, modal content |
| **Geist Mono** | 400â€“800 | Stats, labels, confidence values, code |

### Color Palette

| Token | Dark Theme | Light Theme | Used For |
|-------|-----------|-------------|----------|
| `--accent` | `#22d3ee` cyan | `#0891b2` teal | Canvas border, predictions, primary actions |
| `--green` | `#34d399` emerald | `#059669` green | Network viz, accuracy chart, success |
| `--violet` | `#a78bfa` purple | `#7c3aed` violet | Letter confidence bars, progress gradient |
| `--amber` | `#fbbf24` gold | `#d97706` amber | Loss chart, training status, warnings |
| `--red` | `#f87171` red | `#dc2626` red | Reset/close actions, competing network paths |

### Layout

3-column grid (400px | flexible | 340px) Ã— 2 rows:

| | Column 1 | Column 2 | Column 3 |
|---|----------|----------|----------|
| **Row 1** | Drawing canvas | Neural network viz | Accuracy chart |
| **Row 2** | Controls + prediction | Confidence bars (47) | Loss chart |

Collapses to single column below 1400px.

---

## Dependencies

```
torch>=2.0.0
torchvision>=0.15.0
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
websockets>=12.0
numpy>=1.24.0
pydantic>=2.0.0
```

Frontend has **zero dependencies** â€” vanilla HTML/CSS/JS with ES modules.

<!-- 
ğŸ“¸ CONFIDENCE BARS â€” Crop to just the confidence bars panel showing a letter prediction.
   Digits in cyan, letters in violet, winning class glowing.
   Save as: assets/confidence-bars.png
-->
![Confidence Bars](assets/confidence-bars.png)

---

## Troubleshooting

| Problem | Fix |
|---------|-----|
| "No Model" after launch | Click Train â†’ set epochs â†’ Start. Model saves to `backend/models/digit_model.pt` |
| Wrong predictions | Delete `backend/models/digit_model.pt` and retrain with 35+ epochs |
| CUDA out of memory | Reduce `batch_size` in `train.py` (try 64) |
| Slow inference (>100ms) | Set `use_tta=False` in `predictor.py` for ~5x speedup |
| 3 FPS on GPU | TTA makes each prediction 5 forward passes. Disable for raw speed |
| WebSocket keeps disconnecting | Check firewall/proxy on port 8000. The client auto-reconnects every 2 seconds |
| EMNIST download fails | Manually download from [EMNIST site](https://www.nist.gov/itl/products-and-services/emnist-dataset) into `data/raw/emnist/` |
| Training stalls on Windows | `dataset.py` uses `persistent_workers=True` â€” if it hangs, reduce `num_workers` to 0 |

---

## License

MIT

---

<p align="center">
  <b>Neural</b><i>Scribe</i> â€” watch a neural network learn to read your handwriting.
</p>